\documentclass[12pt,a4paper]{article}
\usepackage{times}
\usepackage{durhampaper}
\usepackage{harvard}
\usepackage{commath,amsmath,amssymb,amsfonts}
\usepackage{bm}
\usepackage{hyperref}
\citationmode{abbr}
\bibliographystyle{agsm}

\title{Inverse Uncertainty Quantification Methods for Numerical Storm Surge Models}
\author{} % leave; your name goes into \student{}
\student{Benjamin Russell}
\supervisor{Tobias Weinzierl}
\degree{MEng Computer Science}

\date{}

\begin{document}

\maketitle

\begin{abstract}
	
{\bf Context/Background} - 	When modelling storm surges using numerical methods, it is important that the model is correctly calibrated to ensure accurate results. Doing this without empirical measurements of the site being simulated is a hard and time consuming problem, and for this reason there exist many different methods for estimating correct model parameters.

{\bf Aims} - This paper aims to compare a variety of different techniques for uncertainty quantification to infer correct parameters. This will allow a comparison between the time taken to estimate the parameters and their accuracy when compared with real data for storm surges. This comparison will highlight which techniques are more appropriate in real world scenarios.

{\bf Method} - The methodology used in this paper was to implement three different techniques one of which is a hybrid of the other two. These were then used to infer and calibrate the model parameter values and the results were then tested in the model on a variety of real world storm surges with their accuracies and runtimes compared.
\end{abstract}

\begin{keywords}
Storm surge, uncertainty quantification, inverse problem, Bayesian inference, Monte-Carlo, Surrogate Model, kriging, polynomial chaos
\end{keywords}

\section{Introduction}
\noindent
When severe storms hit coastal regions, the high winds and atmospheric pressure can cause a sea level rise. This affect is known as a storm surge. Storm surges pose a grave danger to coastal communities both from an economic and a human perspective. From 1963 to 2012 $49\%$ of all deaths caused by hurricanes in the US were due to the subsequent storm surge (REF DEATH PAPER). It has also been estimated that if coastal flood defences are not improved in US coastal regions then by 2050 then yearly losses could exceed US\$1trillion (REF COST PAPER).  To address this issue it is necessary to plan and implement better coastal flood defences. One such way to plan this new infrastructure is to use numerical modelling techniques to simulate hypothetical storm surge events, induced by hurricanes, and study the simulated sea level rise to assess coastal regions most at risk to loss of human life and loss of property. This poses a problem as storm surge models are very complex and their accuracy depends inherently on how well calibrated the input parameters are. To address this, without having accurate measurements of the underlying phenomena governing these the parameters, it is necessary to estimate them and calibrate the model accordingly. Unfortunately due to the complex nature of the numerical models this is time-prohibited and therefore more sophisticated methods of estimation and calibration are used to infer these parameters and quantify the uncertainty of them.

In this paper we focus on using a range of different techniques that fall under the umbrella of surrogate modelling. This is the use of less accurate models that are less computationally expensive to run then the full model (REF QUIRANTE). The remainder of this section will give an overview of storm surge modelling, surrogate modelling, inverse uncertainty quantification, and model calibration as well as the motivations for comparing the various techniques.

\subsection{Background}
\noindent
Simulating storm surges is often done using 2D shallow water equations given as (REF EQUATION PAPER).
\begin{equation}
	\pd{{\bf U}}{t}+\pd{{\bf F(U)}}{x}+\pd{{\bf G(U)}}{y}={\bf S}_\tau+{\bf S}_b
\end{equation}
where ${\bf U}=(h,hu,hv)^T$ this represents the conserved variables according to the three-dimensional conservation laws, with $h$ being the $z$ coordinate (flow depth) and $u, v$ representing the velocity vector averaged by depth. With flux functions ${\bf F}$, ${\bf G}$ defined as:
\begin{align}
	{\bf F} &= \left( hu,hu^2+\frac{1}{2}g_\psi h^2,huv \right) \\
	{\bf G} &= \left( hv,huv,hv^2+\frac{1}{2}g_\psi h^2 \right)
\end{align}
where $g_\psi=g\cos^2\psi$ with $\psi$ being the cosine direction of the seabed's normal. ${\bf S}_\tau$ describes the frictional forces caused by the seabed and is parameterised according to Manning's friction law and an empirically determined $n$ called a Manning roughness coefficient (REF MANNING PAPER). These equations are then solved using finite volume methods to calculate how water depth changes over time against a discretised spacial mesh.

The Manning roughness $n$ must be determined empirically using real world measurements. This is infeasible for a general model for simulating storm surge as it would require these experimental values for $n$ to be determined at each location you would like to simulate. For this reason we use inverse uncertainty quantification to perform parameter calibration. This is done using a Bayesian model starting with a prior distribution of roughness coefficients. This allows for the simulated results to be compared with the real-world results and update the parameters accordingly until they have been estimated to a desired level of confidence. The precise formulation of this Bayesian approach will be explained in a later section. In order to infer the final model parameters the Bayesian model must be sampled repeatedly and the sampled values for $n$ are then used in the simulation with the new results used to further refine the Bayesian model. This sampling is usually performed using Markov chain Monte Carlo (REF MCMC POPULAR), this is, however, computationally expensive. The requirement of running the simulation multiple times in this approach is massively time prohibitive when using complex models such as those for storm surge simulation, which is why surrogate modelling is used as ``grey-box'' substitute.

Surrogate modelling is the use of a simplified model that approximates the outcome of the actual model. In the context of this paper it is a simplified model to approximate the outcome of the storm surge simulation. The use of such a surrogate model is necessary due to the long runtime of an actual simulation run (REF Tsunami paper). Designing a good surrogate model is difficult and must be done in way that approximates well otherwise the runtime benefit is not worth it and will result in inaccurate parameter calibration, discussed previously. There are many different approaches to surrogate modelling such as kriging (REF Kriging paper) and support vector machines. In this paper we consider kriging, an alternative method, and a hybrid of the two.

\subsection{Motivation}
\noindent
As previously discussed the use of full numerical models for parameter calibration and inference is infeasible due to the runtime cost of repeated model runs when performing Bayesian inference. For this reason the primary way of improving parameter calibration is to create more accurate and faster surrogate models. It is also possible to perform parameter calibration using the \textit{adjoint method} in which an analytical inverse problem is formulated, however this can be extremely complicated for large numerical models such as storm surge simulation and is therefore beyond the scope of this paper. For this reason in this paper we focus on the surrogate modelling approach to parameter calibration.

The commonly used methods for surrogate modelling draw from a wide range of mathematical disciplines such statistics and even machine learning. In this paper we focus on the statistical methods for surrogate modelling of storm surge phenomena in order to determine their efficacy for this problem.

\subsection{Objectives}
\noindent
In this paper we aim to answer the following research question: \textit{How can surrogate modelling be used to calibrate storm surge model parameters?} We have divided the outcomes of the research into three categories: minimum, intermediate, and advanced.
\begin{itemize}
	\item
	\textbf{Minimum: } Implement and analyse the efficacy of kriging to approximate the result of a numerical storm surge simulation. This includes implementing a storm surge simulation as well using a pre-existing software library.
	\item
	\textbf{Intermediate:} Implement and analyse the efficacy of polynomial chaos (REF PolyChaos) as a surrogate modelling technique and compare the results to those of kriging.
	\item
	\textbf{Advanced:} Implement the proposed hybrid technique \textit{polynomial chaos kriging} proposed by (REF PC-Kriging) and compare the results to the previous methods. As well as analyse the contributions posed by the constituent methods (kriging, and polynomial chaos) to the final result of the hybrid method. Finally we will conclude whether or not the hybrids result is dominated by one of the constituent methods giving only a negligible increase in either accuracy or decrease in runtime when compared to a single method.
\end{itemize}

The layout of the rest of this paper is as follows: section \ref{sec:lit} presents a survey of related and background literature on surrogate modelling for Bayesian inference in the context of parameter calibration, section \ref{sec:sol} describes the proposed solution and methodology to answer the proposed research question, and section \ref{sec:val} addresses how the proposed solution and literature review have been carried out in  way that ensures any findings drawn from this research are valid and reproducible.

\section{Related Work} \label{sec:lit}
This section presents a survey of existing work on the problems that this project addresses.  it should be between 2 to 4 pages in length.  The rest of this section shows the formats of subsections as well as some general formatting information for tables, figures, references and equations.

\section{Solution} \label{sec:sol}

This section presents the solutions to the problems in detail.  The design and implementation details should all be placed in this section.  You may create a number of subsections, each focussing on one issue.  

This section should be between 4 to 7 pages in length.

\section{Validity} \label{sec:val}

\bibliography{projectpaper}


\end{document}