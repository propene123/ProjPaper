\documentclass[12pt,a4paper]{article}
\usepackage{times}
\usepackage{durhampaper}
\usepackage{harvard}
\usepackage{commath,amsmath,amssymb,amsfonts}
\usepackage{bm}
\usepackage{hyperref}
\citationmode{abbr}
\bibliographystyle{agsm}

\title{Inverse Uncertainty Quantification Methods for Numerical Storm Surge Models}
\author{} % leave; your name goes into \student{}
\student{Benjamin Russell}
\supervisor{Tobias Weinzierl}
\degree{MEng Computer Science}

\date{}

\begin{document}

\maketitle

\begin{abstract}
	
{\bf Context/Background} - 	When modelling storm surges using numerical methods, it is important that the model is correctly calibrated to ensure accurate results. Doing this without empirical measurements of the site being simulated is a hard and time consuming problem, and for this reason there exist many different methods for estimating correct model parameters.

{\bf Aims} - This paper aims to compare a variety of different techniques for uncertainty quantification to infer correct parameters. This will allow a comparison between the time taken to estimate the parameters and their accuracy when compared with real data for storm surges. This comparison will highlight which techniques are more appropriate in real world scenarios.

{\bf Method} - The methodology used in this paper was to implement three different techniques one of which is a hybrid of the other two. These were then used to infer and calibrate the model parameter values and the results were then tested in the model on a variety of real world storm surges with their accuracies and runtimes compared.
\end{abstract}

\begin{keywords}
Storm surge, uncertainty quantification, inverse problem, Bayesian inference, Monte-Carlo, Surrogate Model, kriging, polynomial chaos
\end{keywords}

\section{Introduction}
\noindent
When severe storms hit coastal regions, the high winds and atmospheric pressure can cause a sea level rise. This affect is known as a storm surge. Storm surges pose a grave danger to coastal communities both from an economic and a human perspective. From 1963 to 2012 $49\%$ of all deaths caused by hurricanes in the US were due to the subsequent storm surge (REF DEATH PAPER). It has also been estimated that if coastal flood defences are not improved in US coastal regions then by 2050 then yearly losses could exceed US\$1trillion (REF COST PAPER).  To address this issue it is necessary to plan and implement better coastal flood defences. One such way to plan this new infrastructure is to use numerical modelling techniques to simulate hypothetical storm surge events, induced by hurricanes, and study the simulated sea level rise to assess coastal regions most at risk to loss of human life and loss of property. This poses a problem as storm surge models are very complex and their accuracy depends inherently on how well calibrated the input parameters are. To address this, without having accurate measurements of the underlying phenomena governing these the parameters, it is necessary to estimate them and calibrate the model accordingly. Unfortunately due to the complex nature of the numerical models this is time-prohibited and therefore more sophisticated methods of estimation and calibration are used to infer these parameters and quantify the uncertainty of them.

In this paper we focus on using a range of different techniques that fall under the umbrella of surrogate modelling. This is the use of less accurate models that are less computationally expensive to run then the full model (REF QUIRANTE). The remainder of this section will give an overview of storm surge modelling, surrogate modelling, inverse uncertainty quantification, and model calibration as well as the motivations for comparing the various techniques.

\subsection{Background}
\noindent
Simulating storm surges is often done using 2D shallow water equations given as (REF EQUATION PAPER).
\begin{equation}
	\pd{{\bf U}}{t}+\pd{{\bf F(U)}}{x}+\pd{{\bf G(U)}}{y}={\bf S}_\tau+{\bf S}_b
\end{equation}
where ${\bf U}=(h,hu,hv)^T$ this represents the conserved variables according to the three-dimensional conservation laws, with $h$ being the $z$ coordinate (flow depth) and $u, v$ representing the velocity vector averaged by depth. With flux functions ${\bf F}$, ${\bf G}$ defined as:
\begin{align}
	{\bf F} &= \left( hu,hu^2+\frac{1}{2}g_\psi h^2,huv \right) \\
	{\bf G} &= \left( hv,huv,hv^2+\frac{1}{2}g_\psi h^2 \right)
\end{align}
where $g_\psi=g\cos^2\psi$ with $\psi$ being the cosine direction of the seabed's normal. ${\bf S}_\tau$ describes the frictional forces caused by the seabed and is parameterised according to Manning's friction law and an empirically determined $n$ called a Manning roughness coefficient (REF MANNING PAPER). These equations are then solved using finite volume methods to calculate how water depth changes over time against a discretised spacial mesh.

The Manning roughness $n$ must be determined empirically using real world measurements. This is infeasible for a general model for simulating storm surge as it would require these experimental values for $n$ to be determined at each location you would like to simulate. For this reason we use inverse uncertainty quantification to perform parameter calibration. This is done using a Bayesian model starting with a prior distribution of roughness coefficients. This allows for the simulated results to be compared with the real-world results and update the parameters accordingly until they have been estimated to a desired level of confidence. The precise formulation of this Bayesian approach will be explained in a later section. In order to infer the final model parameters the Bayesian model must be sampled repeatedly and the sampled values for $n$ are then used in the simulation with the new results used to further refine the Bayesian model. This sampling is usually performed using Markov chain Monte Carlo (MCMC) (REF MCMC POPULAR), this is, however, computationally expensive. The requirement of running the simulation multiple times in this approach is massively time prohibitive when using complex models such as those for storm surge simulation, which is why surrogate modelling is used as ``grey-box'' substitute.

Surrogate modelling is the use of a simplified model that approximates the outcome of the actual model. In the context of this paper it is a simplified model to approximate the outcome of the storm surge simulation. The use of such a surrogate model is necessary due to the long runtime of an actual simulation run (REF Tsunami paper). Designing a good surrogate model is difficult and must be done in way that approximates well otherwise the runtime benefit is not worth it and will result in inaccurate parameter calibration, discussed previously. There are many different approaches to surrogate modelling such as kriging (REF Kriging paper) and support vector machines. In this paper we consider kriging, an alternative method, and a hybrid of the two.

\subsection{Motivation}
\noindent
As previously discussed the use of full numerical models for parameter calibration and inference is infeasible due to the runtime cost of repeated model runs when performing Bayesian inference. For this reason the primary way of improving parameter calibration is to create more accurate and faster surrogate models. It is also possible to perform parameter calibration using the \textit{adjoint method} in which an analytical inverse problem is formulated, however this can be extremely complicated for large numerical models such as storm surge simulation and is therefore beyond the scope of this paper. For this reason in this paper we focus on the surrogate modelling approach to parameter calibration.

The commonly used methods for surrogate modelling draw from a wide range of mathematical disciplines such statistics and even machine learning. In this paper we focus on the statistical methods for surrogate modelling of storm surge phenomena in order to determine their efficacy for this problem.

\subsection{Objectives}
\noindent
In this paper we aim to answer the following research question: \textit{How can surrogate modelling be used to calibrate storm surge model parameters?} We have divided the outcomes of the research into three categories: minimum, intermediate, and advanced.
\begin{itemize}
	\item
	\textbf{Minimum: } Implement and analyse the efficacy of kriging to approximate the result of a numerical storm surge simulation. This includes implementing a storm surge simulation as well using a pre-existing software library.
	\item
	\textbf{Intermediate:} Implement and analyse the efficacy of polynomial chaos (REF PolyChaos) as a surrogate modelling technique and compare the results to those of kriging.
	\item
	\textbf{Advanced:} Implement the proposed hybrid technique \textit{polynomial chaos kriging} proposed by (REF PC-Kriging) and compare the results to the previous methods. As well as analyse the contributions posed by the constituent methods (kriging, and polynomial chaos) to the final result of the hybrid method. Finally we will conclude whether or not the hybrids result is dominated by one of the constituent methods giving only a negligible increase in either accuracy or decrease in runtime when compared to a single method.
\end{itemize}

The layout of the rest of this paper is as follows: section \ref{sec:lit} presents a survey of related and background literature on surrogate modelling for Bayesian inference in the context of parameter calibration, section \ref{sec:sol} describes the proposed solution and methodology to answer the proposed research question, and section \ref{sec:val} addresses how the proposed solution and literature review have been carried out in  way that ensures any findings drawn from this research are valid and reproducible.

\section{Related Work} \label{sec:lit}
\noindent
In this section we provide an overview of the existing literature around Bayesian inference and surrogate modelling in the domain of numerical simulation of complex physical phenomena. There are a very broad range of approaches to solving this problem therefore since out paper focuses on the statistical approaches rather than analytical approaches the literature review will reflect this.

\subsection{Gaussian Process Emulation}
\noindent
One of the more common methods of surrogate modelling is the aforementioned kriging. This method requires the construction of a Gaussian process  emulator (GPE). A GPE operates under the assumption that the outputs of the model follows some multivariate Gaussian distribution. A set of known inputs and outputs can be used to train the GPE such that a set of basis functions are produced that can be easily evaluated to get an approximation of the model output (REF GPE BOOK). It has been shown that in complex models such as those for flood inundation, the empirical variation between predicted model outputs in the GPE is greater than the theoretical difference (REF Flood Paper) meaning that unexplained inaccuracies can be present in a GPE. However (REF Flood Paper) did also show that using a GPE allows for Bayesian inference to be used for model calibration in highly complex hydrodynamic models. The variation in GPE outputs when used for Bayesian calibration was demonstrated by (REF CFD Paper) to be of comparable accuracy to performing full model runs with a significant improvement in runtime, although this was done in the domain of particulate flow simulation which is still a complex numerical problem it may not be directly comparable to storm surge. (REF VALID PAPER) presented a series of diagnostics that can be applied to a GPE in order to determine its effectiveness at approximating model outputs. One of these metrics is the Mahalanobis Distance which allows measuring the number of standard deviations a point is away from the mean in a multidimensional case (REF Mahalanobis). These metrics are significant as they compare the GPE output variation with that of the actual model rather than reality, this is important for out application as our GPE should approximate the model specifically rather than the underlying physical processes.

\subsection{Polynomial Chaos Expansion}
\noindent
Polynomial chaos expansion is another popular method for surrogate modelling. In this method the model is represented as a quantity of interest that can be approximated by an infinite of polynomial coefficients are elements of an orthogonal probability space (REF Xiu Paper). (REF Xiu2 Paper) shows that it is necessary to truncate this infinite series and discusses proofs of the convergence of the series for different selections of polynomials. (REF tsunami paper) used a polynomial chaos surrogate (PC surrogate) to simulate sea elevation rises during tsunamis, constructing a fourth order PC surrogate that only required 125 full model runs to create, and with the maximum error (when compared to the full model) of only 1\%. The computational savings from a PC surrogate can be huge which is highly beneficial when performing MCMC (REF PC Cost Paper) this paper demonstrated up to a 30,000 times reduction in computational cost when using a PC surrogate to perform Bayesian inference for parameter calibration. The paper did find that as the truncation order of the PC series increases then the speedup gets worse, however as (REF tsunami) showed for sea elevation an order 4 truncation is sufficient. A further study into using PC surrogates for calibration of Manning coefficients showed that the RMSE of the surrogate was close to zero at all points except for a few outliers and even then it was less than one standard deviation of error (REF PC RMSE PAPER). As a surrogate method polynomial chaos has some advantages (REF PC adv paper) found some advantages for it: the number of model runs for training is relatively low, uncertainty in the model parameters can easily be incorporated into the model. (REF PC adv paper) also showed that if the parameter uncertainty is assumed to distributed as a standard normal then there exists a very efficient training procedure for the surrogate.

\subsection{Hybrid Models}
\noindent

\section{Solution} \label{sec:sol}

This section presents the solutions to the problems in detail.  The design and implementation details should all be placed in this section.  You may create a number of subsections, each focussing on one issue.  

This section should be between 4 to 7 pages in length.

\section{Validity} \label{sec:val}

\bibliography{projectpaper}


\end{document}